{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLU_tokenization_example.ipynb","provenance":[{"file_id":"1pgqoRJ6yGWbTLWdLnRvwG5DLSU3rxuMq","timestamp":1599401652794},{"file_id":"1JrlfuV2jNGTdOXvaWIoHTSf6BscDMkN7","timestamp":1599401257319},{"file_id":"1svpqtC3cY6JnRGeJngIPl2raqxdowpyi","timestamp":1599400881246},{"file_id":"1tW833T3HS8F5Lvn6LgeDd5LW5226syKN","timestamp":1599398724652},{"file_id":"1CYzHfQyFCdvIOVO2Z5aggVI9c0hDEOrw","timestamp":1599354735581}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"rBXrqlGEYA8G"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/text_pre_processing_and_cleaning/NLU_tokenization_example.ipynb)\n","\n","# Tokenization with NLU \n","\n","Tokenization is the process of splitting input texts into segments which corrospond to words.    \n","\n","I. e. 'He was hungry' consists of the tokens [He,was,hungry]\n","\n","\n","\n","\n","\n","# 1. Install Java and NLU"]},{"cell_type":"code","metadata":{"id":"M2-GiYL6xurJ"},"source":["\n","import os\n","! apt-get update -qq > /dev/null   \n","# Install java\n","! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n","\n","! pip install nlu  pyspark==2.4.7 > /dev/null\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N_CL8HZ8Ydry"},"source":["## 2. Load Model and lemmatize sample string"]},{"cell_type":"code","metadata":{"id":"j2ZZZvr1uGpx","colab":{"base_uri":"https://localhost:8080/","height":111},"executionInfo":{"status":"ok","timestamp":1610864093109,"user_tz":-60,"elapsed":98493,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"34a134de-4e8f-4bd3-ed4b-52c32210b17c"},"source":["import nlu\n","pipe = nlu.load('tokenize')\n","pipe.predict('He was suprised by the diversity of NLU')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>document</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>He was suprised by the diversity of NLU</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             document\n","origin_index                                         \n","0             He was suprised by the diversity of NLU"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"IRSEzc-RCceu"},"source":["# 3. Get one row per token by setting outputlevel to token.    "]},{"cell_type":"code","metadata":{"id":"9bujAZtOCfRW","colab":{"base_uri":"https://localhost:8080/","height":328},"executionInfo":{"status":"ok","timestamp":1610864094929,"user_tz":-60,"elapsed":100308,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"940bc6e2-2453-45ff-de79-cd4eb35a3b11"},"source":["pipe.predict('He was suprised by the diversity of NLU', output_level='token')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>token</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>He</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>was</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>suprised</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>by</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>diversity</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>of</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>NLU</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  token\n","origin_index           \n","0                    He\n","0                   was\n","0              suprised\n","0                    by\n","0                   the\n","0             diversity\n","0                    of\n","0                   NLU"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"uXb-FMA6mX13"},"source":["# 4. Checkout possible configurations for the Tokenizer"]},{"cell_type":"code","metadata":{"id":"9qUF7jPlme-R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610864094930,"user_tz":-60,"elapsed":100303,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"9d2cf52e-2282-44ce-81bf-2da461c8b70a"},"source":["pipe.print_info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The following parameters are configurable for this NLU pipeline (You can copy paste the examples) :\n",">>> pipe['default_tokenizer'] has settable params:\n","pipe['default_tokenizer'].setTargetPattern('\\S+')        | Info: pattern to grab from text as token candidates. Defaults \\S+ | Currently set to : \\S+\n","pipe['default_tokenizer'].setContextChars(['.', ',', ';', ':', '!', '?', '*', '-', '(', ')', '\"', \"'\"])  | Info: character list used to separate from token boundaries | Currently set to : ['.', ',', ';', ':', '!', '?', '*', '-', '(', ')', '\"', \"'\"]\n","pipe['default_tokenizer'].setCaseSensitiveExceptions(True)  | Info: Whether to care for case sensitiveness in exceptions | Currently set to : True\n","pipe['default_tokenizer'].setMinLength(0)                | Info: Set the minimum allowed legth for each token | Currently set to : 0\n","pipe['default_tokenizer'].setMaxLength(99999)            | Info: Set the maximum allowed legth for each token | Currently set to : 99999\n",">>> pipe['document_assembler'] has settable params:\n","pipe['document_assembler'].setCleanupMode('shrink')      | Info: possible values: disabled, inplace, inplace_full, shrink, shrink_full, each, each_full, delete_full | Currently set to : shrink\n",">>> pipe['sentence_detector'] has settable params:\n","pipe['sentence_detector'].setUseAbbreviations(True)      | Info: whether to apply abbreviations at sentence detection | Currently set to : True\n","pipe['sentence_detector'].setDetectLists(True)           | Info: whether detect lists during sentence detection | Currently set to : True\n","pipe['sentence_detector'].setUseCustomBoundsOnly(False)  | Info: Only utilize custom bounds in sentence detection | Currently set to : False\n","pipe['sentence_detector'].setCustomBounds([])            | Info: characters used to explicitly mark sentence bounds | Currently set to : []\n","pipe['sentence_detector'].setExplodeSentences(False)     | Info: whether to explode each sentence into a different row, for better parallelization. Defaults to false. | Currently set to : False\n","pipe['sentence_detector'].setMinLength(0)                | Info: Set the minimum allowed length for each sentence. | Currently set to : 0\n","pipe['sentence_detector'].setMaxLength(99999)            | Info: Set the maximum allowed length for each sentence | Currently set to : 99999\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ON37vb9KmnJ2"},"source":["# 4.1 Configure  Context Chars  \n","By defining custom context chars, we can get extra tokens from suffixes that match the context chars. \n"]},{"cell_type":"code","metadata":{"id":"iD376MeemfZG","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1610864096469,"user_tz":-60,"elapsed":101837,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"0af13b19-1a24-4647-dcf2-6b60d0a16c05"},"source":["pipe['default_tokenizer'].setContextChars([',','!','o','d'])\n","pipe.predict('Hello, world!')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>token</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Hello</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>,</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>world</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>!</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              token\n","origin_index       \n","0             Hello\n","0                 ,\n","0             world\n","0                 !"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"Aen1EcOQnmYf"},"source":[""],"execution_count":null,"outputs":[]}]}