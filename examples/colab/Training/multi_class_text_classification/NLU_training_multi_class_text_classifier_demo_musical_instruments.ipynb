{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLU_training_multi_class_text_classifier_demo_musical_instruments.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zkufh760uvF3"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/multi_class_text_classification/NLU_training_multi_class_text_classifier_demo_musical_instruments.ipynb)\n","\n","\n","\n","\n","# Training a Deep Learning Classifier with NLU \n","## ClassifierDL (Multi-class Text Classification)\n","With the [ClassifierDL model](https://nlp.johnsnowlabs.com/docs/en/annotators#classifierdl-multi-class-text-classification) from Spark NLP you can achieve State Of the Art results on any multi class text classification problem \n","\n","This notebook showcases the following features : \n","\n","- How to train the deep learning classifier\n","- How to store a pipeline to disk\n","- How to load the pipeline from disk (Enables NLU offline mode)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dur2drhW5Rvi"},"source":["# 1. Install Java 8 and NLU"]},{"cell_type":"code","metadata":{"id":"hFGnBCHavltY"},"source":["import os\n","! apt-get update -qq > /dev/null   \n","# Install java\n","! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n","! pip install nlu > /dev/null\n","! pip install  pyspark==2.4.7 > /dev/null\n","\n","import nlu"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f4KkTfnR5Ugg"},"source":["# 2. Download musical instruments  classification dataset\r\n","\r\n","https://www.kaggle.com/eswarchandt/amazon-music-reviews\r\n","\r\n","dataset with products rated between 5 classes"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OrVb5ZMvvrQD","executionInfo":{"status":"ok","timestamp":1610787881309,"user_tz":-300,"elapsed":1350,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"c7f2277f-e7a9-484a-cf3b-457bdc65e457"},"source":["! wget http://ckl-it.de/wp-content/uploads/2021/01/Musical_instruments_reviews.csv"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-01-16 09:04:04--  http://ckl-it.de/wp-content/uploads/2021/01/Musical_instruments_reviews.csv\n","Resolving ckl-it.de (ckl-it.de)... 217.160.0.108, 2001:8d8:100f:f000::209\n","Connecting to ckl-it.de (ckl-it.de)|217.160.0.108|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 51708 (50K) [text/csv]\n","Saving to: ‘Musical_instruments_reviews.csv’\n","\n","Musical_instruments 100%[===================>]  50.50K   241KB/s    in 0.2s    \n","\n","2021-01-16 09:04:05 (241 KB/s) - ‘Musical_instruments_reviews.csv’ saved [51708/51708]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y4xSRWIhwT28","executionInfo":{"status":"ok","timestamp":1610787895917,"user_tz":-300,"elapsed":1017,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"1c4ca0c9-9489-47b9-a85e-3a8a3cc092a6"},"source":["import pandas as pd\n","test_path = '/content/Musical_instruments_reviews.csv'\n","train_df = pd.read_csv(test_path,sep=\",\")\n","cols = [\"y\",\"text\"]\n","train_df = train_df[cols]\n","train_df\n","\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>y</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>good</td>\n","      <td>Hosa products are a good bang for the buck. I ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>average</td>\n","      <td>I now use this cable to run from the output of...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>good</td>\n","      <td>Cheap and good texture rubber that does not ge...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>average</td>\n","      <td>These cables are a little thin compared to hos...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>average</td>\n","      <td>It is a decent cable. It does its job, but it ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>115</th>\n","      <td>very poor</td>\n","      <td>It just randomly pops off my bass, it's so sli...</td>\n","    </tr>\n","    <tr>\n","      <th>116</th>\n","      <td>very good</td>\n","      <td>The primary job of this device is to block the...</td>\n","    </tr>\n","    <tr>\n","      <th>117</th>\n","      <td>good</td>\n","      <td>The Hosa XLR cables are affordable and very he...</td>\n","    </tr>\n","    <tr>\n","      <th>118</th>\n","      <td>average</td>\n","      <td>It's a cable, no frills, tangles pretty easy a...</td>\n","    </tr>\n","    <tr>\n","      <th>119</th>\n","      <td>very poor</td>\n","      <td>It hums, crackles, and I think I'm having prob...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>120 rows × 2 columns</p>\n","</div>"],"text/plain":["             y                                               text\n","0         good  Hosa products are a good bang for the buck. I ...\n","1      average  I now use this cable to run from the output of...\n","2         good  Cheap and good texture rubber that does not ge...\n","3      average  These cables are a little thin compared to hos...\n","4      average  It is a decent cable. It does its job, but it ...\n","..         ...                                                ...\n","115  very poor  It just randomly pops off my bass, it's so sli...\n","116  very good  The primary job of this device is to block the...\n","117       good  The Hosa XLR cables are affordable and very he...\n","118    average  It's a cable, no frills, tangles pretty easy a...\n","119  very poor  It hums, crackles, and I think I'm having prob...\n","\n","[120 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"0296Om2C5anY"},"source":["# 3. Train Deep Learning Classifier using nlu.load('train.classifier')\n","\n","By default, the Universal Sentence Encoder Embeddings (USE) are beeing downloaded to provide embeddings for the classifier. You can use any of the 50+ other sentence Emeddings in NLU tough!\n","\n","You dataset label column should be named 'y' and the feature column with text data should be named 'text'"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":671},"id":"3ZIPkRkWftBG","executionInfo":{"status":"ok","timestamp":1609472199891,"user_tz":-300,"elapsed":191855,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"bd493b9c-fa33-44af-e941-1000f0aa137d"},"source":["# load a trainable pipeline by specifying the train. prefix  and fit it on a datset with label and text columns\n","# Since there are no\n","\n","trainable_pipe = nlu.load('train.classifier')\n","fitted_pipe = trainable_pipe.fit(train_df.iloc[:50] )\n","\n","\n","# predict with the trainable pipeline on dataset and get predictions\n","preds = fitted_pipe.predict(train_df.iloc[:50] )\n","preds"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tfhub_use download started this may take some time.\n","Approximate size to download 923.7 MB\n","[OK!]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>category_confidence</th>\n","      <th>text</th>\n","      <th>y</th>\n","      <th>category</th>\n","      <th>default_name_embeddings</th>\n","      <th>sentence</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.304148</td>\n","      <td>Hosa products are a good bang for the buck. I ...</td>\n","      <td>good</td>\n","      <td>average</td>\n","      <td>[0.07208353281021118, 0.028736615553498268, -0...</td>\n","      <td>Hosa products are a good bang for the buck.</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>1.000000</td>\n","      <td>Hosa products are a good bang for the buck. I ...</td>\n","      <td>good</td>\n","      <td>average</td>\n","      <td>[0.056614313274621964, -0.04707420617341995, -...</td>\n","      <td>I haven't looked up the specifications, but I'...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.956961</td>\n","      <td>I now use this cable to run from the output of...</td>\n","      <td>average</td>\n","      <td>average</td>\n","      <td>[0.06778458505868912, -0.0052166287787258625, ...</td>\n","      <td>I now use this cable to run from the output of...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.000000</td>\n","      <td>I now use this cable to run from the output of...</td>\n","      <td>average</td>\n","      <td>average</td>\n","      <td>[0.06371542811393738, -0.022252758964896202, -...</td>\n","      <td>After I bought Monster Cable to hook up my ped...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2.000000</td>\n","      <td>I now use this cable to run from the output of...</td>\n","      <td>average</td>\n","      <td>average</td>\n","      <td>[0.018308864906430244, 0.0024022769648581743, ...</td>\n","      <td>I had been using a high end Planet Waves cable...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>0.841045</td>\n","      <td>Update: The right angle switched end started d...</td>\n","      <td>average</td>\n","      <td>average</td>\n","      <td>[-0.013615701347589493, -0.04160430282354355, ...</td>\n","      <td>I like knowing that.</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>0.841045</td>\n","      <td>Update: The right angle switched end started d...</td>\n","      <td>average</td>\n","      <td>average</td>\n","      <td>[0.02372647449374199, 0.04573449119925499, -0....</td>\n","      <td>** EDIT: AS STATED ABOVE, YOU WILL NOT BE ABLE...</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>0.997217</td>\n","      <td>Doe's not stay on to well, moves to much even ...</td>\n","      <td>average</td>\n","      <td>average</td>\n","      <td>[0.08493339270353317, 0.047714825719594955, -0...</td>\n","      <td>Doe's not stay on to well, moves to much even ...</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>0.401975</td>\n","      <td>These are not the greatest but they're cheap a...</td>\n","      <td>good</td>\n","      <td>very poor</td>\n","      <td>[0.03083745203912258, 0.01701708696782589, -0....</td>\n","      <td>These are not the greatest but they're cheap a...</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>1.000000</td>\n","      <td>These are not the greatest but they're cheap a...</td>\n","      <td>good</td>\n","      <td>very poor</td>\n","      <td>[0.06084448844194412, 0.0020018713548779488, 0...</td>\n","      <td>I've only had one fail and I've bought many of...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>297 rows × 6 columns</p>\n","</div>"],"text/plain":["             category_confidence  ...                                           sentence\n","origin_index                      ...                                                   \n","0                       0.304148  ...        Hosa products are a good bang for the buck.\n","0                       1.000000  ...  I haven't looked up the specifications, but I'...\n","1                       0.956961  ...  I now use this cable to run from the output of...\n","1                       1.000000  ...  After I bought Monster Cable to hook up my ped...\n","1                       2.000000  ...  I had been using a high end Planet Waves cable...\n","...                          ...  ...                                                ...\n","47                      0.841045  ...                               I like knowing that.\n","47                      0.841045  ...  ** EDIT: AS STATED ABOVE, YOU WILL NOT BE ABLE...\n","48                      0.997217  ...  Doe's not stay on to well, moves to much even ...\n","49                      0.401975  ...  These are not the greatest but they're cheap a...\n","49                      1.000000  ...  I've only had one fail and I've bought many of...\n","\n","[297 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"DL_5aY9b3jSd"},"source":["# 4. Evaluate the model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"djtoZVKBw2WU","executionInfo":{"status":"ok","timestamp":1609472199894,"user_tz":-300,"elapsed":191838,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"c3c86659-f624-486c-bb48-f514ac1e8fc0"},"source":["from sklearn.metrics import classification_report\n","print(classification_report(preds['y'], preds['category']))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","     average       0.63      0.76      0.69       123\n","        good       0.00      0.00      0.00        51\n","   very good       0.00      0.00      0.00        39\n","   very poor       0.50      0.87      0.63        84\n","\n","    accuracy                           0.56       297\n","   macro avg       0.28      0.41      0.33       297\n","weighted avg       0.40      0.56      0.46       297\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mhFKVN93o1ZO"},"source":["# 5. Lets try different Sentence Emebddings"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CzJd8omao0gt","executionInfo":{"status":"ok","timestamp":1609472199895,"user_tz":-300,"elapsed":191822,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"0257d8c7-ce4a-4ac4-837c-5513639da2d4"},"source":["# We can use nlu.print_components(action='embed_sentence') to see every possibler sentence embedding we could use. Lets use bert!\n","nlu.print_components(action='embed_sentence')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["For language <en> NLU provides the following Models : \n","nlu.load('en.embed_sentence') returns Spark NLP model tfhub_use\n","nlu.load('en.embed_sentence.use') returns Spark NLP model tfhub_use\n","nlu.load('en.embed_sentence.tfhub_use') returns Spark NLP model tfhub_use\n","nlu.load('en.embed_sentence.use.lg') returns Spark NLP model tfhub_use_lg\n","nlu.load('en.embed_sentence.tfhub_use.lg') returns Spark NLP model tfhub_use_lg\n","nlu.load('en.embed_sentence.albert') returns Spark NLP model albert_base_uncased\n","nlu.load('en.embed_sentence.electra') returns Spark NLP model sent_electra_small_uncased\n","nlu.load('en.embed_sentence.electra_small_uncased') returns Spark NLP model sent_electra_small_uncased\n","nlu.load('en.embed_sentence.electra_base_uncased') returns Spark NLP model sent_electra_base_uncased\n","nlu.load('en.embed_sentence.electra_large_uncased') returns Spark NLP model sent_electra_large_uncased\n","nlu.load('en.embed_sentence.bert') returns Spark NLP model sent_bert_base_uncased\n","nlu.load('en.embed_sentence.bert_base_uncased') returns Spark NLP model sent_bert_base_uncased\n","nlu.load('en.embed_sentence.bert_base_cased') returns Spark NLP model sent_bert_base_cased\n","nlu.load('en.embed_sentence.bert_large_uncased') returns Spark NLP model sent_bert_large_uncased\n","nlu.load('en.embed_sentence.bert_large_cased') returns Spark NLP model sent_bert_large_cased\n","nlu.load('en.embed_sentence.biobert.pubmed_base_cased') returns Spark NLP model sent_biobert_pubmed_base_cased\n","nlu.load('en.embed_sentence.biobert.pubmed_large_cased') returns Spark NLP model sent_biobert_pubmed_large_cased\n","nlu.load('en.embed_sentence.biobert.pmc_base_cased') returns Spark NLP model sent_biobert_pmc_base_cased\n","nlu.load('en.embed_sentence.biobert.pubmed_pmc_base_cased') returns Spark NLP model sent_biobert_pubmed_pmc_base_cased\n","nlu.load('en.embed_sentence.biobert.clinical_base_cased') returns Spark NLP model sent_biobert_clinical_base_cased\n","nlu.load('en.embed_sentence.biobert.discharge_base_cased') returns Spark NLP model sent_biobert_discharge_base_cased\n","nlu.load('en.embed_sentence.covidbert.large_uncased') returns Spark NLP model sent_covidbert_large_uncased\n","nlu.load('en.embed_sentence.small_bert_L2_128') returns Spark NLP model sent_small_bert_L2_128\n","nlu.load('en.embed_sentence.small_bert_L4_128') returns Spark NLP model sent_small_bert_L4_128\n","nlu.load('en.embed_sentence.small_bert_L6_128') returns Spark NLP model sent_small_bert_L6_128\n","nlu.load('en.embed_sentence.small_bert_L8_128') returns Spark NLP model sent_small_bert_L8_128\n","nlu.load('en.embed_sentence.small_bert_L10_128') returns Spark NLP model sent_small_bert_L10_128\n","nlu.load('en.embed_sentence.small_bert_L12_128') returns Spark NLP model sent_small_bert_L12_128\n","nlu.load('en.embed_sentence.small_bert_L2_256') returns Spark NLP model sent_small_bert_L2_256\n","nlu.load('en.embed_sentence.small_bert_L4_256') returns Spark NLP model sent_small_bert_L4_256\n","nlu.load('en.embed_sentence.small_bert_L6_256') returns Spark NLP model sent_small_bert_L6_256\n","nlu.load('en.embed_sentence.small_bert_L8_256') returns Spark NLP model sent_small_bert_L8_256\n","nlu.load('en.embed_sentence.small_bert_L10_256') returns Spark NLP model sent_small_bert_L10_256\n","nlu.load('en.embed_sentence.small_bert_L12_256') returns Spark NLP model sent_small_bert_L12_256\n","nlu.load('en.embed_sentence.small_bert_L2_512') returns Spark NLP model sent_small_bert_L2_512\n","nlu.load('en.embed_sentence.small_bert_L4_512') returns Spark NLP model sent_small_bert_L4_512\n","nlu.load('en.embed_sentence.small_bert_L6_512') returns Spark NLP model sent_small_bert_L6_512\n","nlu.load('en.embed_sentence.small_bert_L8_512') returns Spark NLP model sent_small_bert_L8_512\n","nlu.load('en.embed_sentence.small_bert_L10_512') returns Spark NLP model sent_small_bert_L10_512\n","nlu.load('en.embed_sentence.small_bert_L12_512') returns Spark NLP model sent_small_bert_L12_512\n","nlu.load('en.embed_sentence.small_bert_L2_768') returns Spark NLP model sent_small_bert_L2_768\n","nlu.load('en.embed_sentence.small_bert_L4_768') returns Spark NLP model sent_small_bert_L4_768\n","nlu.load('en.embed_sentence.small_bert_L6_768') returns Spark NLP model sent_small_bert_L6_768\n","nlu.load('en.embed_sentence.small_bert_L8_768') returns Spark NLP model sent_small_bert_L8_768\n","nlu.load('en.embed_sentence.small_bert_L10_768') returns Spark NLP model sent_small_bert_L10_768\n","nlu.load('en.embed_sentence.small_bert_L12_768') returns Spark NLP model sent_small_bert_L12_768\n","For language <fi> NLU provides the following Models : \n","nlu.load('fi.embed_sentence') returns Spark NLP model sent_bert_finnish_cased\n","nlu.load('fi.embed_sentence.bert.cased') returns Spark NLP model sent_bert_finnish_cased\n","nlu.load('fi.embed_sentence.bert.uncased') returns Spark NLP model sent_bert_finnish_uncased\n","For language <xx> NLU provides the following Models : \n","nlu.load('xx.embed_sentence') returns Spark NLP model sent_bert_multi_cased\n","nlu.load('xx.embed_sentence.bert') returns Spark NLP model sent_bert_multi_cased\n","nlu.load('xx.embed_sentence.bert.cased') returns Spark NLP model sent_bert_multi_cased\n","nlu.load('xx.embed_sentence.labse') returns Spark NLP model labse\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ABHLgirmG1n9","executionInfo":{"status":"ok","timestamp":1609472351316,"user_tz":-300,"elapsed":343219,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"b4823a3d-fcf8-4e40-e6dd-00051347b3a8"},"source":["# Load pipe with bert embeds\n","# using large embeddings can take a few hours..\n","# fitted_pipe = nlu.load('en.embed_sentence.bert_large_uncased train.classifier').fit(train_df)\n","fitted_pipe = nlu.load('en.embed_sentence.bert train.classifier').fit(train_df.iloc[:100])\n","\n","\n","# predict with the trained pipeline on dataset and get predictions\n","preds = fitted_pipe.predict(train_df.iloc[:100])\n","from sklearn.metrics import classification_report\n","print(classification_report(preds['y'], preds['category']))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["sent_bert_base_uncased download started this may take some time.\n","Approximate size to download 392.5 MB\n","[OK!]\n","              precision    recall  f1-score   support\n","\n","     average       0.29      1.00      0.45        27\n","        good       0.00      0.00      0.00        25\n","   very good       0.00      0.00      0.00        25\n","   very poor       1.00      0.30      0.47        23\n","\n","    accuracy                           0.34       100\n","   macro avg       0.32      0.33      0.23       100\n","weighted avg       0.31      0.34      0.23       100\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nbpdZGoZPslz","executionInfo":{"status":"ok","timestamp":1609472368869,"user_tz":-300,"elapsed":360758,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"a9574559-a655-464a-f159-b85c2c64b5b0"},"source":["# Load pipe with bert embeds\n","fitted_pipe = nlu.load('embed_sentence.bert train.classifier').fit(train_df.iloc[:100])\n","\n","# predict with the trained pipeline on dataset and get predictions\n","preds = fitted_pipe.predict(train_df.iloc[:100])\n","from sklearn.metrics import classification_report\n","print(classification_report(preds['y'], preds['category']))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["sent_small_bert_L2_128 download started this may take some time.\n","Approximate size to download 16.1 MB\n","[OK!]\n","              precision    recall  f1-score   support\n","\n","     average       0.00      0.00      0.00        27\n","        good       0.00      0.00      0.00        25\n","   very good       0.25      1.00      0.40        25\n","   very poor       0.00      0.00      0.00        23\n","\n","    accuracy                           0.25       100\n","   macro avg       0.06      0.25      0.10       100\n","weighted avg       0.06      0.25      0.10       100\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wYV7ivdsQY8Z","executionInfo":{"status":"ok","timestamp":1609475397624,"user_tz":-300,"elapsed":155002,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"759ff4c2-dcf3-4f65-bf3b-95f1f32e0a39"},"source":["from sklearn.metrics import classification_report\r\n","trainable_pipe = nlu.load('en.embed_sentence.small_bert_L12_768 train.classifier')\r\n","# We need to train longer and user smaller LR for NON-USE based sentence embeddings usually\r\n","# We could tune the hyperparameters further with hyperparameter tuning methods like gridsearch\r\n","# Also longer training gives more accuracy\r\n","trainable_pipe['classifier_dl'].setMaxEpochs(90)  \r\n","trainable_pipe['classifier_dl'].setLr(0.0005) \r\n","fitted_pipe = trainable_pipe.fit(train_df)\r\n","# predict with the trainable pipeline on dataset and get predictions\r\n","preds = fitted_pipe.predict(train_df,output_level='document')\r\n","\r\n","#sentence detector that is part of the pipe generates sone NaNs. lets drop them first\r\n","preds.dropna(inplace=True)\r\n","print(classification_report(preds['y'], preds['category']))\r\n","\r\n","#preds"],"execution_count":null,"outputs":[{"output_type":"stream","text":["sent_small_bert_L12_768 download started this may take some time.\n","Approximate size to download 392.9 MB\n","[OK!]\n","              precision    recall  f1-score   support\n","\n","     average       0.89      0.53      0.67        30\n","        good       0.62      0.83      0.71        30\n","   very good       0.93      0.47      0.62        30\n","   very poor       0.62      0.97      0.75        30\n","\n","    accuracy                           0.70       120\n","   macro avg       0.77      0.70      0.69       120\n","weighted avg       0.77      0.70      0.69       120\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2BB-NwZUoHSe"},"source":["# 5. Lets save the model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eLex095goHwm","executionInfo":{"status":"ok","timestamp":1609472722793,"user_tz":-300,"elapsed":714653,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"4b052d0c-f581-4c96-a7d5-91885525e96e"},"source":["stored_model_path = './models/classifier_dl_trained' \n","fitted_pipe.save(stored_model_path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Stored model in ./models/classifier_dl_trained\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qeuzjy2IJTif"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"e_b2DPd4rCiU"},"source":["# 6. Lets load the model from HDD.\n","This makes Offlien NLU usage possible!   \n","You need to call nlu.load(path=path_to_the_pipe) to load a model/pipeline from disk."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":127},"id":"SO4uz45MoRgp","executionInfo":{"status":"ok","timestamp":1609472740229,"user_tz":-300,"elapsed":732057,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"c57f3bd7-4590-4a82-9d01-99b7dd1e7a34"},"source":["hdd_pipe = nlu.load(path=stored_model_path)\n","\n","preds = hdd_pipe.predict('It was really good ')\n","preds"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting on empty Dataframe, could not infer correct training method!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>document</th>\n","      <th>en_embed_sentence_small_bert_L12_768_embeddings</th>\n","      <th>classifier_confidence</th>\n","      <th>classifier</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>It was really good</td>\n","      <td>[-0.034663598984479904, 0.3307220935821533, 0....</td>\n","      <td>0.529977</td>\n","      <td>very good</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                        document  ... classifier\n","origin_index                      ...           \n","0             It was really good  ...  very good\n","\n","[1 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e0CVlkk9v6Qi","executionInfo":{"status":"ok","timestamp":1609472740233,"user_tz":-300,"elapsed":732044,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"a85b252c-2f3a-401d-8579-de7c2c9acbc1"},"source":["hdd_pipe.print_info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The following parameters are configurable for this NLU pipeline (You can copy paste the examples) :\n",">>> pipe['document_assembler'] has settable params:\n","pipe['document_assembler'].setCleanupMode('shrink')                              | Info: possible values: disabled, inplace, inplace_full, shrink, shrink_full, each, each_full, delete_full | Currently set to : shrink\n",">>> pipe['sentence_detector'] has settable params:\n","pipe['sentence_detector'].setCustomBounds([])                                    | Info: characters used to explicitly mark sentence bounds | Currently set to : []\n","pipe['sentence_detector'].setDetectLists(True)                                   | Info: whether detect lists during sentence detection | Currently set to : True\n","pipe['sentence_detector'].setExplodeSentences(False)                             | Info: whether to explode each sentence into a different row, for better parallelization. Defaults to false. | Currently set to : False\n","pipe['sentence_detector'].setMaxLength(99999)                                    | Info: Set the maximum allowed length for each sentence | Currently set to : 99999\n","pipe['sentence_detector'].setMinLength(0)                                        | Info: Set the minimum allowed length for each sentence. | Currently set to : 0\n","pipe['sentence_detector'].setUseAbbreviations(True)                              | Info: whether to apply abbreviations at sentence detection | Currently set to : True\n","pipe['sentence_detector'].setUseCustomBoundsOnly(False)                          | Info: Only utilize custom bounds in sentence detection | Currently set to : False\n",">>> pipe['regex_tokenizer'] has settable params:\n","pipe['regex_tokenizer'].setCaseSensitiveExceptions(True)                         | Info: Whether to care for case sensitiveness in exceptions | Currently set to : True\n","pipe['regex_tokenizer'].setTargetPattern('\\S+')                                  | Info: pattern to grab from text as token candidates. Defaults \\S+ | Currently set to : \\S+\n","pipe['regex_tokenizer'].setMaxLength(99999)                                      | Info: Set the maximum allowed length for each token | Currently set to : 99999\n","pipe['regex_tokenizer'].setMinLength(0)                                          | Info: Set the minimum allowed length for each token | Currently set to : 0\n",">>> pipe['glove'] has settable params:\n","pipe['glove'].setBatchSize(32)                                                   | Info: Batch size. Large values allows faster processing but requires more memory. | Currently set to : 32\n","pipe['glove'].setCaseSensitive(False)                                            | Info: whether to ignore case in tokens for embeddings matching | Currently set to : False\n","pipe['glove'].setDimension(768)                                                  | Info: Number of embedding dimensions | Currently set to : 768\n","pipe['glove'].setMaxSentenceLength(128)                                          | Info: Max sentence length to process | Currently set to : 128\n","pipe['glove'].setIsLong(False)                                                   | Info: Use Long type instead of Int type for inputs buffer - Some Bert models require Long instead of Int. | Currently set to : False\n","pipe['glove'].setStorageRef('sent_small_bert_L12_768')                           | Info: unique reference name for identification | Currently set to : sent_small_bert_L12_768\n",">>> pipe['classifier_dl'] has settable params:\n","pipe['classifier_dl'].setClasses(['very good', 'very poor', 'average', 'good'])  | Info: get the tags used to trained this NerDLModel | Currently set to : ['very good', 'very poor', 'average', 'good']\n","pipe['classifier_dl'].setStorageRef('sent_small_bert_L12_768')                   | Info: unique reference name for identification | Currently set to : sent_small_bert_L12_768\n"],"name":"stdout"}]}]}