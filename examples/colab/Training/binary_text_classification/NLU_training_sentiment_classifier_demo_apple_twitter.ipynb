{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLU_training_sentiment_classifier_demo_apple_twitter.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"RIV-9vEqxTBB"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\r\n","\r\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/binary_text_classification/NLU_training_sentiment_classifier_demo_apple_twitter.ipynb)\r\n","\r\n","\r\n","\r\n","# Training a Sentiment Analysis Classifier with NLU \r\n","With the [SentimentDL model](https://nlp.johnsnowlabs.com/docs/en/annotators#sentimentdl-multi-class-sentiment-analysis-annotator) from Spark NLP you can achieve State Of the Art results on any multi class text classification problem \r\n","\r\n","This notebook showcases the following features : \r\n","\r\n","- How to train the deep learning classifier\r\n","- How to store a pipeline to disk\r\n","- How to load the pipeline from disk (Enables NLU offline mode)\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"05-mAOF6ol-0"},"source":["import os\r\n","from sklearn.metrics import classification_report\r\n","! apt-get update -qq > /dev/null   \r\n","# Install java\r\n","! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\r\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n","os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\r\n","! pip install nlu pyspark==2.4.7 > /dev/null  \r\n","\r\n","\r\n","import nlu"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f4KkTfnR5Ugg"},"source":["# 2. Download appple twitter  Sentiment dataset \n","https://www.kaggle.com/seriousran/appletwittersentimenttexts\n","\n","this dataset contains tweets made towards apple and today we are going to train our model to predict whether the tweet contains sentiment!\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OrVb5ZMvvrQD","executionInfo":{"status":"ok","timestamp":1609468082890,"user_tz":-300,"elapsed":77740,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"a791d4cf-bfa3-4cc6-a60d-c885afe2e917"},"source":["! wget https://raw.githubusercontent.com/ahmedlone127/nlu-master/main/apple-twitter-sentiment-texts.csv\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-01-01 02:27:38--  https://raw.githubusercontent.com/ahmedlone127/nlu-master/main/apple-twitter-sentiment-texts.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 31678 (31K) [text/plain]\n","Saving to: ‘apple-twitter-sentiment-texts.csv’\n","\n","apple-twitter-senti 100%[===================>]  30.94K  --.-KB/s    in 0.002s  \n","\n","2021-01-01 02:27:39 (12.9 MB/s) - ‘apple-twitter-sentiment-texts.csv’ saved [31678/31678]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":415},"id":"y4xSRWIhwT28","executionInfo":{"status":"ok","timestamp":1609468083287,"user_tz":-300,"elapsed":78124,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"1a23969f-abf0-4bc3-e2ec-0879b2b77cad"},"source":["import pandas as pd\n","train_path = '/content/apple-twitter-sentiment-texts.csv'\n","\n","train_df = pd.read_csv(train_path)\n","# the text data to use for classification should be in a column named 'text'\n","# the label column must have name 'y' name be of type str\n","columns=['text','y']\n","train_df = train_df[columns]\n","train_df = train_df[~train_df[\"y\"].isin([\"neuteral\"])]\n","train_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>@Apple  you need to sort your phones out.</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Wow. Yall needa step it up @Apple RT @heynyla:...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I'm surprised there isn't more talk about what...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Realised the reason @apple make huge phones is...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Apple Inc. CEO Donates $291K To Pennsylvania S...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>281</th>\n","      <td>@apple so thanks for being greedy assholes who...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>282</th>\n","      <td>@apple iCal AGAIN!!! it reset all my recurring...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>283</th>\n","      <td>Just did my first transaction with @Apple Pay ...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>284</th>\n","      <td>RT @JPDesloges: Kantar Worldpanel: iPhone sale...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>285</th>\n","      <td>Yeeaaayyy....awesome OS X Yosemite 10.10.1 roc...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>286 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                  text         y\n","0            @Apple  you need to sort your phones out.  negative\n","1    Wow. Yall needa step it up @Apple RT @heynyla:...  negative\n","2    I'm surprised there isn't more talk about what...  negative\n","3    Realised the reason @apple make huge phones is...  negative\n","4    Apple Inc. CEO Donates $291K To Pennsylvania S...  positive\n","..                                                 ...       ...\n","281  @apple so thanks for being greedy assholes who...  negative\n","282  @apple iCal AGAIN!!! it reset all my recurring...  negative\n","283  Just did my first transaction with @Apple Pay ...  positive\n","284  RT @JPDesloges: Kantar Worldpanel: iPhone sale...  positive\n","285  Yeeaaayyy....awesome OS X Yosemite 10.10.1 roc...  positive\n","\n","[286 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"0296Om2C5anY"},"source":["# 3. Train Deep Learning Classifier using nlu.load('train.sentiment')\n","\n","You dataset label column should be named 'y' and the feature column with text data should be named 'text'"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":845},"id":"3ZIPkRkWftBG","executionInfo":{"status":"ok","timestamp":1609468191792,"user_tz":-300,"elapsed":186618,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"41d18f44-64e1-4766-a8cf-4545813930d7"},"source":["import nlu \n","# load a trainable pipeline by specifying the train. prefix  and fit it on a datset with label and text columns\n","# by default the Universal Sentence Encoder (USE) Sentence embeddings are used for generation\n","trainable_pipe = nlu.load('train.sentiment')\n","fitted_pipe = trainable_pipe.fit(train_df.iloc[:50])\n","\n","# predict with the trainable pipeline on dataset and get predictions\n","preds = fitted_pipe.predict(train_df.iloc[:50],output_level='document')\n","#sentence detector that is part of the pipe generates sone NaNs. lets drop them first\n","preds.dropna(inplace=True)\n","print(classification_report(preds['y'], preds['sentiment']))\n","\n","preds"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tfhub_use download started this may take some time.\n","Approximate size to download 923.7 MB\n","[OK!]\n","              precision    recall  f1-score   support\n","\n","    negative       0.91      0.80      0.85       143\n","     neutral       0.00      0.00      0.00         0\n","    positive       0.82      0.91      0.86       143\n","\n","    accuracy                           0.86       286\n","   macro avg       0.58      0.57      0.57       286\n","weighted avg       0.86      0.86      0.86       286\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentiment_confidence</th>\n","      <th>y</th>\n","      <th>default_name_embeddings</th>\n","      <th>text</th>\n","      <th>document</th>\n","      <th>sentiment</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.998447</td>\n","      <td>negative</td>\n","      <td>[-0.01731022447347641, 0.010604134760797024, -...</td>\n","      <td>@Apple  you need to sort your phones out.</td>\n","      <td>@Apple you need to sort your phones out.</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.990570</td>\n","      <td>negative</td>\n","      <td>[0.019931159913539886, -0.04991159215569496, -...</td>\n","      <td>Wow. Yall needa step it up @Apple RT @heynyla:...</td>\n","      <td>Wow. Yall needa step it up @Apple RT @heynyla:...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.969844</td>\n","      <td>negative</td>\n","      <td>[0.01646081730723381, -0.02681073546409607, -0...</td>\n","      <td>I'm surprised there isn't more talk about what...</td>\n","      <td>I'm surprised there isn't more talk about what...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.996128</td>\n","      <td>negative</td>\n","      <td>[0.04638500511646271, -0.037105873227119446, -...</td>\n","      <td>Realised the reason @apple make huge phones is...</td>\n","      <td>Realised the reason @apple make huge phones is...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.959235</td>\n","      <td>positive</td>\n","      <td>[-0.028623634949326515, 0.03947276994585991, -...</td>\n","      <td>Apple Inc. CEO Donates $291K To Pennsylvania S...</td>\n","      <td>Apple Inc. CEO Donates $291K To Pennsylvania S...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>281</th>\n","      <td>0.978435</td>\n","      <td>negative</td>\n","      <td>[0.03778046742081642, 0.03407461196184158, 0.0...</td>\n","      <td>@apple so thanks for being greedy assholes who...</td>\n","      <td>@apple so thanks for being greedy assholes who...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>282</th>\n","      <td>0.623791</td>\n","      <td>negative</td>\n","      <td>[-0.013547728769481182, -0.001025827950797975,...</td>\n","      <td>@apple iCal AGAIN!!! it reset all my recurring...</td>\n","      <td>@apple iCal AGAIN!!! it reset all my recurring...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>283</th>\n","      <td>0.999104</td>\n","      <td>positive</td>\n","      <td>[-0.0015363194979727268, -0.01644994132220745,...</td>\n","      <td>Just did my first transaction with @Apple Pay ...</td>\n","      <td>Just did my first transaction with @Apple Pay ...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>284</th>\n","      <td>0.999854</td>\n","      <td>positive</td>\n","      <td>[0.0656985342502594, 0.028557728976011276, -0....</td>\n","      <td>RT @JPDesloges: Kantar Worldpanel: iPhone sale...</td>\n","      <td>RT @JPDesloges: Kantar Worldpanel: iPhone sale...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>285</th>\n","      <td>0.983244</td>\n","      <td>positive</td>\n","      <td>[0.02311933971941471, 0.05785432830452919, -0....</td>\n","      <td>Yeeaaayyy....awesome OS X Yosemite 10.10.1 roc...</td>\n","      <td>Yeeaaayyy....awesome OS X Yosemite 10.10.1 roc...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>286 rows × 6 columns</p>\n","</div>"],"text/plain":["             sentiment_confidence  ... sentiment\n","origin_index                       ...          \n","0                        0.998447  ...  negative\n","1                        0.990570  ...  positive\n","2                        0.969844  ...  negative\n","3                        0.996128  ...  negative\n","4                        0.959235  ...  positive\n","...                           ...  ...       ...\n","281                      0.978435  ...  negative\n","282                      0.623791  ...  positive\n","283                      0.999104  ...  positive\n","284                      0.999854  ...  positive\n","285                      0.983244  ...  positive\n","\n","[286 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"lVyOE2wV0fw_"},"source":["# Test the fitted pipe on new example"]},{"cell_type":"code","metadata":{"id":"qdCUg2MR0PD2","colab":{"base_uri":"https://localhost:8080/","height":110},"executionInfo":{"status":"ok","timestamp":1609468194339,"user_tz":-300,"elapsed":189158,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"00d8c7b6-22e1-4979-8c51-58471540a3dd"},"source":["fitted_pipe.predict('I hate the newest update')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentiment_confidence</th>\n","      <th>default_name_embeddings</th>\n","      <th>document</th>\n","      <th>sentiment</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.996097</td>\n","      <td>[0.06468033790588379, -0.040837567299604416, -...</td>\n","      <td>Bitcoin is going to the moon!</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             sentiment_confidence  ... sentiment\n","origin_index                       ...          \n","0                        0.996097  ...  positive\n","\n","[1 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"xflpwrVjjBVD"},"source":["## Configure pipe training parameters"]},{"cell_type":"code","metadata":{"id":"UtsAUGTmOTms","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609468194341,"user_tz":-300,"elapsed":189154,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"3ab00ec5-5894-400f-c6c9-e32099fed1f5"},"source":["trainable_pipe.print_info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The following parameters are configurable for this NLU pipeline (You can copy paste the examples) :\n",">>> pipe['sentiment_dl'] has settable params:\n","pipe['sentiment_dl'].setMaxEpochs(2)                 | Info: Maximum number of epochs to train | Currently set to : 2\n","pipe['sentiment_dl'].setLr(0.005)                    | Info: Learning Rate | Currently set to : 0.005\n","pipe['sentiment_dl'].setBatchSize(64)                | Info: Batch size | Currently set to : 64\n","pipe['sentiment_dl'].setDropout(0.5)                 | Info: Dropout coefficient | Currently set to : 0.5\n","pipe['sentiment_dl'].setEnableOutputLogs(True)       | Info: Whether to use stdout in addition to Spark logs. | Currently set to : True\n","pipe['sentiment_dl'].setThreshold(0.6)               | Info: The minimum threshold for the final result otheriwse it will be neutral | Currently set to : 0.6\n","pipe['sentiment_dl'].setThresholdLabel('neutral')    | Info: In case the score is less than threshold, what should be the label. Default is neutral. | Currently set to : neutral\n",">>> pipe['sentence_detector'] has settable params:\n","pipe['sentence_detector'].setUseAbbreviations(True)  | Info: whether to apply abbreviations at sentence detection | Currently set to : True\n","pipe['sentence_detector'].setDetectLists(True)       | Info: whether detect lists during sentence detection | Currently set to : True\n","pipe['sentence_detector'].setUseCustomBoundsOnly(False)  | Info: Only utilize custom bounds in sentence detection | Currently set to : False\n","pipe['sentence_detector'].setCustomBounds([])        | Info: characters used to explicitly mark sentence bounds | Currently set to : []\n","pipe['sentence_detector'].setExplodeSentences(False)  | Info: whether to explode each sentence into a different row, for better parallelization. Defaults to false. | Currently set to : False\n","pipe['sentence_detector'].setMinLength(0)            | Info: Set the minimum allowed length for each sentence. | Currently set to : 0\n","pipe['sentence_detector'].setMaxLength(99999)        | Info: Set the maximum allowed length for each sentence | Currently set to : 99999\n",">>> pipe['default_tokenizer'] has settable params:\n","pipe['default_tokenizer'].setTargetPattern('\\S+')    | Info: pattern to grab from text as token candidates. Defaults \\S+ | Currently set to : \\S+\n","pipe['default_tokenizer'].setContextChars(['.', ',', ';', ':', '!', '?', '*', '-', '(', ')', '\"', \"'\"])  | Info: character list used to separate from token boundaries | Currently set to : ['.', ',', ';', ':', '!', '?', '*', '-', '(', ')', '\"', \"'\"]\n","pipe['default_tokenizer'].setCaseSensitiveExceptions(True)  | Info: Whether to care for case sensitiveness in exceptions | Currently set to : True\n","pipe['default_tokenizer'].setMinLength(0)            | Info: Set the minimum allowed legth for each token | Currently set to : 0\n","pipe['default_tokenizer'].setMaxLength(99999)        | Info: Set the maximum allowed legth for each token | Currently set to : 99999\n",">>> pipe['default_name'] has settable params:\n","pipe['default_name'].setDimension(512)               | Info: Number of embedding dimensions | Currently set to : 512\n","pipe['default_name'].setStorageRef('tfhub_use')      | Info: unique reference name for identification | Currently set to : tfhub_use\n",">>> pipe['document_assembler'] has settable params:\n","pipe['document_assembler'].setCleanupMode('shrink')  | Info: possible values: disabled, inplace, inplace_full, shrink, shrink_full, each, each_full, delete_full | Currently set to : shrink\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2GJdDNV9jEIe"},"source":["## Retrain with new parameters"]},{"cell_type":"code","metadata":{"id":"mptfvHx-MMMX","colab":{"base_uri":"https://localhost:8080/","height":793},"executionInfo":{"status":"ok","timestamp":1609468205048,"user_tz":-300,"elapsed":199854,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"9c9a1628-3034-4be0-94bc-7c109d2c3263"},"source":["# Train longer!\n","trainable_pipe['sentiment_dl'].setMaxEpochs(5)  \n","fitted_pipe = trainable_pipe.fit(train_df.iloc[:100])\n","# predict with the trainable pipeline on dataset and get predictions\n","preds = fitted_pipe.predict(train_df.iloc[:100],output_level='document')\n","\n","#sentence detector that is part of the pipe generates sone NaNs. lets drop them first\n","preds.dropna(inplace=True)\n","print(classification_report(preds['y'], preds['sentiment']))\n","\n","preds"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","    negative       0.96      0.85      0.90       143\n","     neutral       0.00      0.00      0.00         0\n","    positive       0.87      0.95      0.91       143\n","\n","    accuracy                           0.90       286\n","   macro avg       0.61      0.60      0.60       286\n","weighted avg       0.92      0.90      0.91       286\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentiment_confidence</th>\n","      <th>y</th>\n","      <th>default_name_embeddings</th>\n","      <th>text</th>\n","      <th>document</th>\n","      <th>sentiment</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.999738</td>\n","      <td>negative</td>\n","      <td>[-0.01731022447347641, 0.010604134760797024, -...</td>\n","      <td>@Apple  you need to sort your phones out.</td>\n","      <td>@Apple you need to sort your phones out.</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.937319</td>\n","      <td>negative</td>\n","      <td>[0.019931159913539886, -0.04991159215569496, -...</td>\n","      <td>Wow. Yall needa step it up @Apple RT @heynyla:...</td>\n","      <td>Wow. Yall needa step it up @Apple RT @heynyla:...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.974594</td>\n","      <td>negative</td>\n","      <td>[0.01646081730723381, -0.02681073546409607, -0...</td>\n","      <td>I'm surprised there isn't more talk about what...</td>\n","      <td>I'm surprised there isn't more talk about what...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.997196</td>\n","      <td>negative</td>\n","      <td>[0.04638500511646271, -0.037105873227119446, -...</td>\n","      <td>Realised the reason @apple make huge phones is...</td>\n","      <td>Realised the reason @apple make huge phones is...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.709098</td>\n","      <td>positive</td>\n","      <td>[-0.028623634949326515, 0.03947276994585991, -...</td>\n","      <td>Apple Inc. CEO Donates $291K To Pennsylvania S...</td>\n","      <td>Apple Inc. CEO Donates $291K To Pennsylvania S...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>281</th>\n","      <td>0.984257</td>\n","      <td>negative</td>\n","      <td>[0.03778046742081642, 0.03407461196184158, 0.0...</td>\n","      <td>@apple so thanks for being greedy assholes who...</td>\n","      <td>@apple so thanks for being greedy assholes who...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>282</th>\n","      <td>0.904880</td>\n","      <td>negative</td>\n","      <td>[-0.013547728769481182, -0.001025827950797975,...</td>\n","      <td>@apple iCal AGAIN!!! it reset all my recurring...</td>\n","      <td>@apple iCal AGAIN!!! it reset all my recurring...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>283</th>\n","      <td>0.995687</td>\n","      <td>positive</td>\n","      <td>[-0.0015363194979727268, -0.01644994132220745,...</td>\n","      <td>Just did my first transaction with @Apple Pay ...</td>\n","      <td>Just did my first transaction with @Apple Pay ...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>284</th>\n","      <td>0.998746</td>\n","      <td>positive</td>\n","      <td>[0.0656985342502594, 0.028557728976011276, -0....</td>\n","      <td>RT @JPDesloges: Kantar Worldpanel: iPhone sale...</td>\n","      <td>RT @JPDesloges: Kantar Worldpanel: iPhone sale...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>285</th>\n","      <td>0.710708</td>\n","      <td>positive</td>\n","      <td>[0.02311933971941471, 0.05785432830452919, -0....</td>\n","      <td>Yeeaaayyy....awesome OS X Yosemite 10.10.1 roc...</td>\n","      <td>Yeeaaayyy....awesome OS X Yosemite 10.10.1 roc...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>286 rows × 6 columns</p>\n","</div>"],"text/plain":["             sentiment_confidence  ... sentiment\n","origin_index                       ...          \n","0                        0.999738  ...  negative\n","1                        0.937319  ...  positive\n","2                        0.974594  ...  negative\n","3                        0.997196  ...  negative\n","4                        0.709098  ...  positive\n","...                           ...  ...       ...\n","281                      0.984257  ...  negative\n","282                      0.904880  ...  negative\n","283                      0.995687  ...  positive\n","284                      0.998746  ...  positive\n","285                      0.710708  ...  positive\n","\n","[286 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"qFoT-s1MjTSS"},"source":["# Try training with different Embeddings"]},{"cell_type":"code","metadata":{"id":"nxWFzQOhjWC8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609468205058,"user_tz":-300,"elapsed":199858,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"11560398-8fb9-4110-aed3-f7d9c1f71268"},"source":["# We can use nlu.print_components(action='embed_sentence') to see every possibler sentence embedding we could use. Lets use bert!\n","nlu.print_components(action='embed_sentence')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["For language <en> NLU provides the following Models : \n","nlu.load('en.embed_sentence') returns Spark NLP model tfhub_use\n","nlu.load('en.embed_sentence.use') returns Spark NLP model tfhub_use\n","nlu.load('en.embed_sentence.tfhub_use') returns Spark NLP model tfhub_use\n","nlu.load('en.embed_sentence.use.lg') returns Spark NLP model tfhub_use_lg\n","nlu.load('en.embed_sentence.tfhub_use.lg') returns Spark NLP model tfhub_use_lg\n","nlu.load('en.embed_sentence.albert') returns Spark NLP model albert_base_uncased\n","nlu.load('en.embed_sentence.electra') returns Spark NLP model sent_electra_small_uncased\n","nlu.load('en.embed_sentence.electra_small_uncased') returns Spark NLP model sent_electra_small_uncased\n","nlu.load('en.embed_sentence.electra_base_uncased') returns Spark NLP model sent_electra_base_uncased\n","nlu.load('en.embed_sentence.electra_large_uncased') returns Spark NLP model sent_electra_large_uncased\n","nlu.load('en.embed_sentence.bert') returns Spark NLP model sent_bert_base_uncased\n","nlu.load('en.embed_sentence.bert_base_uncased') returns Spark NLP model sent_bert_base_uncased\n","nlu.load('en.embed_sentence.bert_base_cased') returns Spark NLP model sent_bert_base_cased\n","nlu.load('en.embed_sentence.bert_large_uncased') returns Spark NLP model sent_bert_large_uncased\n","nlu.load('en.embed_sentence.bert_large_cased') returns Spark NLP model sent_bert_large_cased\n","nlu.load('en.embed_sentence.biobert.pubmed_base_cased') returns Spark NLP model sent_biobert_pubmed_base_cased\n","nlu.load('en.embed_sentence.biobert.pubmed_large_cased') returns Spark NLP model sent_biobert_pubmed_large_cased\n","nlu.load('en.embed_sentence.biobert.pmc_base_cased') returns Spark NLP model sent_biobert_pmc_base_cased\n","nlu.load('en.embed_sentence.biobert.pubmed_pmc_base_cased') returns Spark NLP model sent_biobert_pubmed_pmc_base_cased\n","nlu.load('en.embed_sentence.biobert.clinical_base_cased') returns Spark NLP model sent_biobert_clinical_base_cased\n","nlu.load('en.embed_sentence.biobert.discharge_base_cased') returns Spark NLP model sent_biobert_discharge_base_cased\n","nlu.load('en.embed_sentence.covidbert.large_uncased') returns Spark NLP model sent_covidbert_large_uncased\n","nlu.load('en.embed_sentence.small_bert_L2_128') returns Spark NLP model sent_small_bert_L2_128\n","nlu.load('en.embed_sentence.small_bert_L4_128') returns Spark NLP model sent_small_bert_L4_128\n","nlu.load('en.embed_sentence.small_bert_L6_128') returns Spark NLP model sent_small_bert_L6_128\n","nlu.load('en.embed_sentence.small_bert_L8_128') returns Spark NLP model sent_small_bert_L8_128\n","nlu.load('en.embed_sentence.small_bert_L10_128') returns Spark NLP model sent_small_bert_L10_128\n","nlu.load('en.embed_sentence.small_bert_L12_128') returns Spark NLP model sent_small_bert_L12_128\n","nlu.load('en.embed_sentence.small_bert_L2_256') returns Spark NLP model sent_small_bert_L2_256\n","nlu.load('en.embed_sentence.small_bert_L4_256') returns Spark NLP model sent_small_bert_L4_256\n","nlu.load('en.embed_sentence.small_bert_L6_256') returns Spark NLP model sent_small_bert_L6_256\n","nlu.load('en.embed_sentence.small_bert_L8_256') returns Spark NLP model sent_small_bert_L8_256\n","nlu.load('en.embed_sentence.small_bert_L10_256') returns Spark NLP model sent_small_bert_L10_256\n","nlu.load('en.embed_sentence.small_bert_L12_256') returns Spark NLP model sent_small_bert_L12_256\n","nlu.load('en.embed_sentence.small_bert_L2_512') returns Spark NLP model sent_small_bert_L2_512\n","nlu.load('en.embed_sentence.small_bert_L4_512') returns Spark NLP model sent_small_bert_L4_512\n","nlu.load('en.embed_sentence.small_bert_L6_512') returns Spark NLP model sent_small_bert_L6_512\n","nlu.load('en.embed_sentence.small_bert_L8_512') returns Spark NLP model sent_small_bert_L8_512\n","nlu.load('en.embed_sentence.small_bert_L10_512') returns Spark NLP model sent_small_bert_L10_512\n","nlu.load('en.embed_sentence.small_bert_L12_512') returns Spark NLP model sent_small_bert_L12_512\n","nlu.load('en.embed_sentence.small_bert_L2_768') returns Spark NLP model sent_small_bert_L2_768\n","nlu.load('en.embed_sentence.small_bert_L4_768') returns Spark NLP model sent_small_bert_L4_768\n","nlu.load('en.embed_sentence.small_bert_L6_768') returns Spark NLP model sent_small_bert_L6_768\n","nlu.load('en.embed_sentence.small_bert_L8_768') returns Spark NLP model sent_small_bert_L8_768\n","nlu.load('en.embed_sentence.small_bert_L10_768') returns Spark NLP model sent_small_bert_L10_768\n","nlu.load('en.embed_sentence.small_bert_L12_768') returns Spark NLP model sent_small_bert_L12_768\n","For language <fi> NLU provides the following Models : \n","nlu.load('fi.embed_sentence') returns Spark NLP model sent_bert_finnish_cased\n","nlu.load('fi.embed_sentence.bert.cased') returns Spark NLP model sent_bert_finnish_cased\n","nlu.load('fi.embed_sentence.bert.uncased') returns Spark NLP model sent_bert_finnish_uncased\n","For language <xx> NLU provides the following Models : \n","nlu.load('xx.embed_sentence') returns Spark NLP model sent_bert_multi_cased\n","nlu.load('xx.embed_sentence.bert') returns Spark NLP model sent_bert_multi_cased\n","nlu.load('xx.embed_sentence.bert.cased') returns Spark NLP model sent_bert_multi_cased\n","nlu.load('xx.embed_sentence.labse') returns Spark NLP model labse\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eLex095goHwm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609468415116,"user_tz":-300,"elapsed":409908,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"b8e4f245-595a-40f3-9e1d-76f71e76b74e"},"source":["trainable_pipe = nlu.load('en.embed_sentence.small_bert_L12_768 train.sentiment')\n","# We need to train longer and user smaller LR for NON-USE based sentence embeddings usually\n","# We could tune the hyperparameters further with hyperparameter tuning methods like gridsearch\n","# Also longer training gives more accuracy\n","trainable_pipe['sentiment_dl'].setMaxEpochs(110)  \n","trainable_pipe['sentiment_dl'].setLr(0.0005) \n","fitted_pipe = trainable_pipe.fit(train_df)\n","# predict with the trainable pipeline on dataset and get predictions\n","preds = fitted_pipe.predict(train_df,output_level='document')\n","\n","#sentence detector that is part of the pipe generates sone NaNs. lets drop them first\n","preds.dropna(inplace=True)\n","print(classification_report(preds['y'], preds['sentiment']))\n","\n","#preds"],"execution_count":null,"outputs":[{"output_type":"stream","text":["sent_small_bert_L12_768 download started this may take some time.\n","Approximate size to download 392.9 MB\n","[OK!]\n","              precision    recall  f1-score   support\n","\n","    negative       0.96      0.85      0.90       143\n","     neutral       0.00      0.00      0.00         0\n","    positive       0.92      0.92      0.92       143\n","\n","    accuracy                           0.88       286\n","   macro avg       0.63      0.59      0.61       286\n","weighted avg       0.94      0.88      0.91       286\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2BB-NwZUoHSe"},"source":["# 5. Lets save the model"]},{"cell_type":"code","metadata":{"id":"bZZpObLOtqo8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609468632998,"user_tz":-300,"elapsed":627783,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"e6a87d34-ce84-4968-c3a0-9aade476874b"},"source":["stored_model_path = './models/classifier_dl_trained' \r\n","fitted_pipe.save(stored_model_path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Stored model in ./models/classifier_dl_trained\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e_b2DPd4rCiU"},"source":["# 6. Lets load the model from HDD.\n","This makes Offlien NLU usage possible!   \n","You need to call nlu.load(path=path_to_the_pipe) to load a model/pipeline from disk."]},{"cell_type":"code","metadata":{"id":"SO4uz45MoRgp","colab":{"base_uri":"https://localhost:8080/","height":127},"executionInfo":{"status":"ok","timestamp":1609468646911,"user_tz":-300,"elapsed":641690,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"454b2c7d-7c32-4cc2-cf25-a52b5a879abd"},"source":["hdd_pipe = nlu.load(path=stored_model_path)\n","\n","preds = hdd_pipe.predict('I hate the newest update')\n","preds"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting on empty Dataframe, could not infer correct training method!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentiment_confidence</th>\n","      <th>en_embed_sentence_small_bert_L12_768_embeddings</th>\n","      <th>document</th>\n","      <th>sentiment</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.974083</td>\n","      <td>[-0.058236218988895416, -0.3061041235923767, 0...</td>\n","      <td>I hate it</td>\n","      <td>negative</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             sentiment_confidence  ... sentiment\n","origin_index                       ...          \n","0                        0.974083  ...  negative\n","\n","[1 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"e0CVlkk9v6Qi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609468646914,"user_tz":-300,"elapsed":641685,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"80ce5918-3803-45f4-e10f-300144342295"},"source":["hdd_pipe.print_info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The following parameters are configurable for this NLU pipeline (You can copy paste the examples) :\n",">>> pipe['document_assembler'] has settable params:\n","pipe['document_assembler'].setCleanupMode('shrink')            | Info: possible values: disabled, inplace, inplace_full, shrink, shrink_full, each, each_full, delete_full | Currently set to : shrink\n",">>> pipe['regex_tokenizer'] has settable params:\n","pipe['regex_tokenizer'].setCaseSensitiveExceptions(True)       | Info: Whether to care for case sensitiveness in exceptions | Currently set to : True\n","pipe['regex_tokenizer'].setTargetPattern('\\S+')                | Info: pattern to grab from text as token candidates. Defaults \\S+ | Currently set to : \\S+\n","pipe['regex_tokenizer'].setMaxLength(99999)                    | Info: Set the maximum allowed length for each token | Currently set to : 99999\n","pipe['regex_tokenizer'].setMinLength(0)                        | Info: Set the minimum allowed length for each token | Currently set to : 0\n",">>> pipe['sentence_detector'] has settable params:\n","pipe['sentence_detector'].setCustomBounds([])                  | Info: characters used to explicitly mark sentence bounds | Currently set to : []\n","pipe['sentence_detector'].setDetectLists(True)                 | Info: whether detect lists during sentence detection | Currently set to : True\n","pipe['sentence_detector'].setExplodeSentences(False)           | Info: whether to explode each sentence into a different row, for better parallelization. Defaults to false. | Currently set to : False\n","pipe['sentence_detector'].setMaxLength(99999)                  | Info: Set the maximum allowed length for each sentence | Currently set to : 99999\n","pipe['sentence_detector'].setMinLength(0)                      | Info: Set the minimum allowed length for each sentence. | Currently set to : 0\n","pipe['sentence_detector'].setUseAbbreviations(True)            | Info: whether to apply abbreviations at sentence detection | Currently set to : True\n","pipe['sentence_detector'].setUseCustomBoundsOnly(False)        | Info: Only utilize custom bounds in sentence detection | Currently set to : False\n",">>> pipe['glove'] has settable params:\n","pipe['glove'].setBatchSize(32)                                 | Info: Batch size. Large values allows faster processing but requires more memory. | Currently set to : 32\n","pipe['glove'].setCaseSensitive(False)                          | Info: whether to ignore case in tokens for embeddings matching | Currently set to : False\n","pipe['glove'].setDimension(768)                                | Info: Number of embedding dimensions | Currently set to : 768\n","pipe['glove'].setMaxSentenceLength(128)                        | Info: Max sentence length to process | Currently set to : 128\n","pipe['glove'].setIsLong(False)                                 | Info: Use Long type instead of Int type for inputs buffer - Some Bert models require Long instead of Int. | Currently set to : False\n","pipe['glove'].setStorageRef('sent_small_bert_L12_768')         | Info: unique reference name for identification | Currently set to : sent_small_bert_L12_768\n",">>> pipe['sentiment_dl'] has settable params:\n","pipe['sentiment_dl'].setThreshold(0.6)                         | Info: The minimum threshold for the final result otheriwse it will be neutral | Currently set to : 0.6\n","pipe['sentiment_dl'].setThresholdLabel('neutral')              | Info: In case the score is less than threshold, what should be the label. Default is neutral. | Currently set to : neutral\n","pipe['sentiment_dl'].setClasses(['positive', 'negative'])      | Info: get the tags used to trained this NerDLModel | Currently set to : ['positive', 'negative']\n","pipe['sentiment_dl'].setStorageRef('sent_small_bert_L12_768')  | Info: unique reference name for identification | Currently set to : sent_small_bert_L12_768\n"],"name":"stdout"}]}]}